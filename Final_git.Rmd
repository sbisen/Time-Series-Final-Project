---
title: "Final_git"
output: html_document
---
#setwd('/Users/shivikakbisen/Desktop/Project_1')
#getwd()
---
title: "Project Report: POMP Model on Foreign Currency Exchange Rate of UK Pound"
date: "4/19/2020"
output:
  html_document:
    fig_caption: true
    theme: flatly
    toc: yes
    toc_depth: 2
    number_sections: true
    pandoc_args: [
      "--number-offset=0"
    ]


# Introduction

* Economists and Mathematicians have been trying to study the foreign exchange rate for years. From some research paper, Geometric Brownian Motion (GBM) has been proved useful in simulating financial data (Brewer, K., Feng, Y., & Kwan, C, 2012). Gerber, for example, points out in the paper that GBM could be used to model assets and liability (Gerber, H., & Shiu, E., 2003). Motivated by those research papaers, I want to model the motion of foreign exchange rate with time series techniques. 

* The methods I use to approach the above question are POMP and GARCH model. I compare the log likelihood of the two models to see which one performs better. 

* In this report, I will focus on the GBP/USD exchange rate. 

* The question of interests is that whether GBM POMP is suitable for fitting the GBP/USD exchange rate data and provides stable results. Also, by comparing the GBM POMP model and GARCH model, which method would be more appropriate for practical use. 
<br>

-------

--------
```{r libraries, echo=FALSE, warning = FALSE, results = 'hide',message=FALSE}
set.seed(100000)
library(ggplot2)
library(plyr)
library(reshape2)
library(pomp)
library(tseries)
library(doParallel)
library(foreach)
library(doMC)
```

# Data Source

* To study the relationships, we attain our data from the website of Federal Reserve System (Foreign Exchange Rate, 2016). 

* The dataset has two variables, Rate (GBP/USD exchange rate) and Date. It is a daily data recording GBP/USD exchange rate from the year 2000 to present. 

* The original data has 4244 observations. Some of them are missing values. The missing data represents that the market is closed for that day. I removed all missing values in the dataset. That is to say, we do not consider the closing market days. After removing missing values, we have 4088 observations. Also to note that there are around 260 data points for each year. 

* For computational purpose, we select only 300 subset of the original data. It records foreign exchange rate around the year 2010 and 2011. The economic market was quite stable at that time. 

* We write ${N_t,t=1,\dots,T}$ for the data. 
<br>
```{r read_data, echo=FALSE,fig.cap="Figure 1. Time Series of Original Data",warning = FALSE}
data = read.csv("https://raw.githubusercontent.com/sbisen/Time-Series-Final-Project/master/rate.csv")
data$Rate = as.numeric(as.character(data$Rate))
# Remove 2790, which is closing market
dt = na.omit(data)
plot(ts(dt$Rate,start = 2000, end = 2016, deltat = 1/260), main="Time Series Plot for GBP/USD from 2000 to 2016",xlab="Year",ylab = 'Exchange Rate',type = 'l')
```

* Figure 1 shows the times series plot of the whole data set. We can see a sharp decrease in the year 2008, which is caused by the Financial Crisis. 

```{r visualize_data, echo=FALSE,fig.cap="Figure 2. Time Series of Used Data"}
# Take the data subset with 400 samples
dt2 = dt[2601:2900,]
dt2$Date=1:300
fx = pomp(dt2,times="Date",t0=0)
fx<- as.data.frame(fx)
plot(fx, xlab = 'time from 0 to 300', main = 'GBP/USD Exchagne Rate')
```

* Figure 2 shows a subset of 300 data points. They are recorded between the year 2010 and 2011. It represents a healthy market. For convenience of computation, we use 0 to 300 to represent the time. There are many fluctuations in this time interval. Overall, it seems that there is an increasing trend. 

-------

-------

# Mathematical Modal

## Geometric Brownian Motion (GBM) Model

* Brewer points out in his paper that the Geometric Brownian Motion Model means the logarithm of the data follows a Brownian motion and provided the following equations(Brewer, K., Feng, Y., & Kwan, C, 2016). 

* The original differential equation is defined as 
$$ dN = \mu Ndt+\delta Ndz$$
where $N$ is the foreign exchange rate on that day, $dz=\epsilon \sqrt{dt}$ and $\epsilon$ is a random draw from the normal distribution with mean 0 and variance $\sigma$. $\mu$ and $\delta$ are usually the drift parameter and the volatility parameter, respectively. 
<br>

* The equation is equivalent to 
$$ d\log(N) = (\mu -\frac{\delta ^2}{2})dt+\delta dz$$

* After solving the differential equation, we get 
$$ N_{t+\Delta{t}}=N_{t}e^{(\mu -\frac{\delta ^2}{2})\Delta{t}+\delta \epsilon \sqrt{\Delta{t}}}$$
Set $\Delta{t}$ equal to 1, we have 
$$ N_{t+1}=N_{t}e^{(\mu -\frac{\delta ^2}{2})\frac{1}{n}+\frac{\delta }{\sqrt{n}}\epsilon }$$
where $n$ is the number of days in a year, which is 260 (only accounts for open market days).
<br>

-------

-------

## Parameter Description

* There are 3 parameters $\mu$, $\delta$ and $\sigma$. 

* $\mu$ is the drift parameter that shows the increasing or decreasing trend. 

* $\delta$ is the volatility parameter that measures the deviations from the mean. 

* $\sigma$ is the variance of the state parameter $\epsilon$. By increasing sigma, it will increase the deviations from the mean. 

-------

-------

# POMP Model

## Build POMP Model

* The rprocess is based on the GBM model with two state variables, $N$ and $\epsilon$.

* The parameters are $\mu$, $\sigma$ and $\delta$.

* The initial value of N is drawn from a random poisson distribution with mean 1.5. The initial value of $\epsilon$ is drawn from a poisson distribution with mean 1. 

* The rmeasure is defined as Rate being drawn from a random draw from the normal distribution with mean 0 and variance $N$, which is the state variable. 

* The detailed implementation is shown below. 

```{r pomp}
dmeas <- Csnippet("lik = dnorm(Rate,0,N,give_log);")
rmeas <- Csnippet("Rate = rnorm(0,N);")
Ne_initializer <- ("N=rpois(1.5);e=rpois(1);")
stochStep <- Csnippet("
                      e = rnorm(0,sigma);
                      N = N*exp((mu-delta*delta/2)/260+delta/sqrt(260)*e);
                      ")

stopifnot(packageVersion("pomp")>="0.75-1")
fx <- pomp(data = dt2,
     times="Date",
     t0=0,
     rprocess=discrete_time(step.fun=stochStep,delta.t=1),
     rmeasure = rmeas,
     dmeasure=dmeas, 
     obsnames = "Rate",
     paramnames=c("mu","delta","sigma"),
     statenames=c("N","e"),
     #initializer=Csnippet(Ne_initializer)
     rinit=Csnippet(Ne_initializer)
     )
```

-------

-------

## Set Run Level

* There are three run levels. The analysis of this report is based on level 3.

* Detailed parameters are defined below

```{r run_level}
run_level = 3
level_Np = c(100,1000,5000)
level_Nmif = c(10,100,300)
level_Nreps_eval = c(4,10,20)
level_Nreps_local = c(10,20,20)
level_Nreps_global = c(10,20,100)
```

# Likelihood Slice

* I first used slicing to get a brief view of when the log likelihood is maximized for each parameter. 
 
```{r slicing, echo = FALSE, results = 'hide'}
p <- sliceDesign(
  c(mu=0.1,delta=0.2,sigma=0.4, N=2600),
  mu=rep(seq(from=-10,to=10,length=40),each=3),
  delta=rep(seq(from=0.1,to=3,length=40),each=3),
  sigma=rep(seq(from=0.1,to=3,length=40),each=3)
  )

registerDoMC(cores=5)
set.seed(998468235L,kind="L'Ecuyer")
mcopts <- list(preschedule=FALSE,set.seed=TRUE)

foreach (theta=iter(p,"row"),.combine=rbind,
         .inorder=FALSE,.options.multicore=mcopts) %dopar% 
         {
           pfilter(fx,params=unlist(theta),Np=5000) -> pf
           #pf
           theta$loglik <- logLik(pf)
           theta
         } -> p
```

```{r mu, echo = FALSE, fig.cap="Figure 3. Slicing for mu"}
v = "mu"
x <- subset(p,slice==v)
plot(x[[v]],x$loglik,xlab=v,ylab="loglik",main='slicing for mu')
```

* Along the $\mu$ direction, the slicing for $\mu$ shows that the maximum of log likelihood is located when $\mu$ is aorund 0. 

```{r delta, echo = FALSE, fig.cap="Figure 4. Slicing for delta"}
v = "delta"
x <- subset(p,slice==v)
plot(x[[v]],x$loglik,xlab=v,ylab="loglik",main='slicing for delta')
```

* Along the $\delta$ direction, the slicing for $\delta$ shows that the maximum of log likelihood is located when $\delta$ is around 0.6.

```{r sigma, echo = FALSE, fig.cap="Figure 5. Slicing for sigma"}
v = "sigma"
x <- subset(p,slice==v)
plot(x[[v]],x$loglik,xlab=v,ylab="loglik",main='slicing for sigma')
```

* Along the $\sigma$ direction, the slicing for $\sigma$ shows that the maximum of log likelihood is located when $\sigma$ is 1.3.

-------

-------

# Partical Filter

```{r filtering, echo = FALSE}
test = c(N.0=1.5,e.0=0,mu=0,delta=0.7,sigma=1.4)
registerDoParallel()
stew(file=sprintf("pf1.rda",run_level),{
  t.pf1 <- system.time(
    pf1 <- foreach(i=1:level_Nreps_eval[run_level],.packages='pomp',.options.multicore=list(set.seed=TRUE)) %dopar% try(
                     pfilter(fx,params=test,
                             Np=level_Np[run_level])
                   )
  )
},seed=493536993,kind="L'Ecuyer")
(L_pf <-logmeanexp(sapply(pf1,logLik),se=TRUE))
```
